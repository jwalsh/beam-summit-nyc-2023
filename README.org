* Beam Summit 2023

- https://beamsummit.org/program/

** Tue, 13 Jun

- 09:00 AM - 09:15 AM: Welcome by Danielle Syse
- 09:15 AM - 09:45 AM: How to Fail with Real-time Analytics by Matthew Housley
- 09:45 AM - 10:30 AM: Beam ML past, present and future by Kerry Donny-Clark & Reza Rokni
- 10:30 AM - 11:00 AM: Break
- 11:00 AM - 11:25 AM: Beam at Talend - the long road from incubator project to cloud-based Pipeline Designer tool by Alexey Romanenko
- 11:00 AM - 11:50 AM: How to write an IO for Beam by John Casey
- 11:00 AM - 11:50 AM: Multi-language pipelines: a unique Beam feature that will make your team more efficient by Chamikara Jayalath
- 11:30 AM - 11:55 AM: Scaling Public Internet Data Collection With Apache Beam by Lior Dadosh
- 12:00 PM - 12:50 PM: A Beginners Guide to Avro and Beam Schemas Without Smashing Your Keyboard by Devon Peticolas
- 12:00 PM - 12:50 PM: Beam IO: CDAP And SparkReceiver IO Connectors Overview by Alex Kosolapov & Elizaveta Lomteva
- 12:00 PM - 12:30 PM: Managed Stream Processing through Apache Beam at LinkedIn by Xinyu Liu, Bingfeng Xia & +1 More Speakers
- 01:00 PM - 02:00 PM: Lunch
- 02:00 PM - 02:25 PM: Easy Cross-Language With SchemaTransforms: Use Your Favorite Java Transform In Python SDK by Ahmed Abualsaud
- 02:00 PM - 02:25 PM: From Dataflow Templates to Beam: Chartboost’s Journey by Austin Bennett & Ferran Fernandez
- 02:30 PM - 02:55 PM: Cross-language JdbcIO enabled by Beam portable schemas by Yi Hu
- 02:30 PM - 02:55 PM: Mapping Data to FHIR with Apache Beam by Alex Fragotsis
- 02:30 PM - 02:55 PM: Meeting Security Requirements For Apache Beam Pipelines On Google Cloud by Lorenzo Caggioni
- 03:00 PM - 03:25 PM: Introduction to Clustering in Apache Beam by Jasper Van den Bossche
- 03:00 PM - 03:25 PM: Oops I *actually* wrote a Portable Beam Runner in Go by Robert Burke
- 03:00 PM - 03:25 PM: Simplifying Speech-to-Text Processing with Apache Beam and Redis by Pramod Rao & Prateek Sheel
- 03:30 PM - 03:55 PM: Developing (experimental) Rust SDKs and a Beam engine for IoT devices by Sho Nakatani
- 03:30 PM - 03:55 PM: Hot Key Detection and Handling in Apache Beam Pipelines by Shafiqa Iqbal & Ikenna Okolo
- 03:30 PM - 03:55 PM: Scaling Up The OpenTelemetry Collector With Beam Go by Alex Van Boxel
- 04:00 PM - 04:15 PM: Break
- 04:15 PM - 04:40 PM: Managing dependencies of Python pipelines by Valentyn Tymofieiev
- 04:15 PM - 04:40 PM: Troubleshooting Slow Running Beam Pipelines by Mehak Gupta
- 04:15 PM - 04:40 PM: Unbreakable & Supercharged Beam Apps with Scala + ZIO by Sahil Khandwala & Aris Vlasakakis
- 04:45 PM - 05:35 PM: Beam loves Kotlin: full pipeline with Kotlin and Midgard library by Mazlum Tosun
- 04:45 PM - 05:45 PM: Community Discussion: Future of Beam by Alex Van Boxel
- 04:45 PM - 05:10 PM: Resolving out of memory issues in Beam Pipelines by Zeeshan Khan
- 05:15 PM - 05:40 PM: Benchmarking Beam pipelines on Dataflow by Pranav Bhandari

** Wed, 14 Jun 

- 09:00 AM - 10:00 AM: Founders' Panel by Federico Patota, Reuven Lax & +2 More Speakers
- 10:00 AM - 10:30 AM: Break
- 10:30 AM - 10:55 AM: Apache Beam and Ensemble Modeling: A Winning Combination for Machine Learning by Shubham Krishna
- 10:30 AM - 10:55 AM: Dealing with order in streams using Apache Beam by Israel Herraiz
- 10:30 AM - 10:55 AM: Running Apache Beam on Kubernetes: A Case Study by Sascha Kerbler
- 11:00 AM - 11:25 AM: Building Fully Managed Service for Beam Jobs with Flink on Kubernetes by Talat Uyarer & Rishabh Kedia
- 11:00 AM - 11:25 AM: Getting started with Apache Beam Quest by Svetak Sundhar
- 11:00 AM - 11:50 AM: Per Entity Training Pipelines in Apache Beam by Jasper Van den Bossche
- 11:30 AM - 11:55 AM: Running Beam Multi Language Pipeline on Flink Cluster on Kubernetes by Lydian Lee
- 11:30 AM - 11:55 AM: Too big to fail - a Beam Pattern for enriching a Stream using State and Timers by Tobias Kaymak & Israel Herraiz
- 12:00 PM - 12:25 PM: Deduplicating and analysing time-series data with Apache Beam and QuestDB by Javier Ramirez
- 12:00 PM - 12:50 PM: How many ways can you skin a cat, if the cat is a problem that needs an ML model to solve? by Kerry Donny-Clark
- 12:00 PM - 12:25 PM: Machine Learning Platform Tooling with Apache Beam on Kubernetes by Charles Adetiloye
- 12:30 PM - 12:55 PM: Design considerations to operate a stateful streaming pipeline as a service by Bhupinder Sindhwani & Israel Herraiz
- 12:30 PM - 01:00 PM: Using Large Language Models in Data Engineering Tasks by Sean Jensen-Grey & Vince Gonzalez
- 01:00 PM - 02:00 PM: Lunch
- 02:00 PM - 02:25 PM: Large scale data processing Using Apache Beam and TFX libraries by Olusayo Olumayode Akinlaja
- 02:00 PM - 02:25 PM: Parallelizing Skewed Hbase Regions using Splittable Dofn by Prathap Reddy
- 02:00 PM - 02:25 PM: Write your own model handler for RunInference! by Ritesh Ghorse
- 02:30 PM - 02:55 PM: Case study: Using statefulDofns to process late arriving data by Amruta Deshmukh
- 02:30 PM - 02:55 PM: How to balance power and control when using Dataflow with an OLTP SQL Database by Florian Bastin & Leo Babonnaud
- 02:30 PM - 02:55 PM: Power Realtime Machine Learning Feature Engineering with Managed Beam at LinkedIn by Yanan Hao & David Shao
- 03:00 PM - 03:50 PM: CI/CD for Dataflow with Flex Templates and Cloud Build by Mazlum Tosun
- 03:00 PM - 03:50 PM: Dataflow Streaming - What's new and what's coming by Tom Stepp & Iñigo San Jose Visiers
- 03:00 PM - 03:25 PM: Optimizing Machine Learning Workloads on Dataflow by Alex Chan
- 03:30 PM - 03:55 PM: ML model updates with side inputs in Dataflow streaming pipelines by Anand Inguva
- 04:00 PM - 04:15 PM: Break
- 04:15 PM - 05:15 PM: Beam Lightning Talks by Pablo Estrada
- 04:15 PM - 04:40 PM: Loading Geospatial data to Google BigQuery by Sean Jensen-Grey & Dong Sun
- 04:15 PM - 04:40 PM: Use Apache Beam to build Machine Learning Feature System at Affirm by Hao Xu
- 04:45 PM - 05:10 PM: Accelerating Machine Learning Predictions with NVIDIA TensorRT and Apache Beam by Shubham Krishna
- 04:45 PM - 05:10 PM: Streamlining Data Engineering and Visualization with Apache Beam and Power BI: A Real-World Case Study by Deexith Reddy
- 05:30 PM - 08:00 PM: AI Camp: Generative AI meetup

** Thu, 15 Jun 

- 09:00 AM - 10:30 AM: Workshop: Application Modernization with Kafka and Beam by Sami Ahmed
- 09:00 AM - 10:30 AM: Workshop: Catch them if you can - Observability and monitoring by Wei Hsia
- 09:00 AM - 10:30 AM: Workshop: Step by step development of a streaming pipeline in Python by Anthony L

* Actions 

- https://www.cloudskillsboost.google/catalog?qlcampaign=1h-opensource-27
- https://www.tensorflow.org/hub/tutorials

** [#A] Beam Examples 

- https://github.com/apache/beam/tree/master/sdks/python/apache_beam/examples

* Session Notes 

** [#C] How to Fail with Real-time Analytics by Matthew Housley

- https://beamsummit.org/sessions/2023/how-to-fail-realtime-analytics/

- Kafka
- Beam 
- Kubernetes cluster 
- SLA/SLO/SLI

** DONE [#B] Beam ML past, present and future by Kerry Donny-Clark & Reza Rokni

#+begin_src python
  with beam.Ppeline as p:
      (p
       | filter Matchfiles
       | map lambda)
#+end_src

- https://github.com/apache/beam/tree/master/examples/notebooks/beam-ml

*** Generative AI

- https://github.com/apache/beam/blob/master/examples/notebooks/beam-ml/run_inference_generative_ai.ipynb (hugging face)

*** TensorFlow Hub

- https://github.com/apache/beam/blob/master/examples/notebooks/beam-ml/run_inference_with_tensorflow_hub.ipynb

- https://www.tensorflow.org/hub

- https://colab.research.google.com/github/apache/beam/blob/master/examples/notebooks/beam-ml/run_inference_with_tensorflow_hub.ipynb

*** Generative Art

#+begin_ai :image :size 256x256
Juan Gris art of Apache Beam conference in NYC (mixed media: newspaper, histograms, flame graph  in oil)
#+end_ai
[[file:images/20230613_256x256_image_13.png]]
[[file:images/20230613_256x256_image_12.png]]
[[file:images/20230613_256x256_image_11.png]]
[[file:images/20230613_256x256_image_10.png]]
[[file:images/20230613_256x256_image_9.png]]
[[file:images/20230613_256x256_image_8.png]]
[[file:images/20230613_256x256_image_7.png]]
[[file:images/20230613_256x256_image_6.png]]
[[file:images/20230613_256x256_image_5.png]]
[[file:images/20230613_256x256_image_4.png]]
[[file:images/20230613_256x256_image_3.png]]
[[file:images/20230613_256x256_image_2.png]]
[[file:images/20230613_256x256_image_1.png]]
[[file:images/20230613_256x256_image.png]]

** [#C] Multi-language pipelines: a unique Beam feature that will make your team more efficient by Chamikara Jayalath
** [#B] Troubleshooting Slow Running Beam Pipelines by Mehak Gupta

- https://beamsummit.org/sessions/2023/troubleshooting-slow-running-beam-pipelines/

- Identification of slow pipelines 
- MTTR metrics

#+begin_src dot
  digraph G {
      logs -> cause 
      logs -> quotas
      ...
  }
#+end_src

- GCP dashboarding 
- worker-startup, worker, docker, kubelet, shuffler
- This is a subset of system, vm-health, vm-monitor, ...
- Job Metrics > throughput 
- Metrics > CPU utilization
- Data freshness
- Batch > Execution Details > straggler detection (reshuffle)
- GC thrashing 
- machine types, decrease parallelism, or dataflow prime

** [#C] Community Discussion: Future of Beam by Alex Van Boxel

- Airflow
- Multi-dimensional watermarks
- DAGS and Beam 
- Time as a concept 
- Using graph databases and iteration with loop unrolling 
- Graph traversal 

** [#C] Founders' Panel by Federico Patota, Reuven Lax & +2 More Speakers
** [#C] Running Apache Beam on Kubernetes: A Case Study by Sascha Kerbler
** [#C] How many ways can you skin a cat, if the cat is a problem that needs an ML model to solve? by Kerry Donny-Clark


#+begin_ai
In Python, scaffold using typing, docstrings, and canonical nomenclature code examples for the following:

I’ll show how Beam’s RunInference transforms lets you use the ML framework of your choice to solve your inference problems. Pytorch, ONNX, TensorFlow, and TensorRT models will be demonstrated.



[AI]: 
```python
from typing import List, Tuple

def run_inference(model_path: str, input_data: List[Tuple[str, str]]) -> List[float]:
    """
    Runs inference using the ML framework of your choice.

    Args:
        model_path (str): Path to the model file.
        input_data (List[Tuple[str, str]]): List of input data in the format [(input_signature, input_data), ...]

    Returns:
        List[float]: List of inference outputs.
    """
    # implementation details

if __name__ == "__main__":
    model_path = "/path/to/model"
    input_data = [("data1", "input1"), ("data2", "input2"), ...]
    output = run_inference(model_path, input_data)
    print(output)
```

Note: In the code above, `input_signature` is a string that represents the shape and datatype of the input data, and `input_data` is a string representing the actual input data. This format can vary depending on the ML framework being used. In addition, the implementation details of the `run_inference` function will vary depending on the chosen framework.

[ME]: Create examples for all models

[AI]: PyTorch example with RunInference transforms:

```python
import torch
from typing import List, Tuple

def pytorch_predict(model_path: str, input_data: List[Tuple[str, torch.Tensor]]) -> List[float]:
    model = torch.jit.load(model_path)
    outputs = []
    for input_signature, input_tensor in input_data:
        output_tensor = model(input_tensor)
        outputs.append(output_tensor.item())
    return outputs

if __name__ == "__main__":
    # Example usage
    model_path = "/path/to/pytorch_model.pt"
    input_data = [("data1", torch.randn(1, 3, 224, 224)), ("data2", torch.randn(1, 3, 224, 224))]
    outputs = pytorch_predict(model_path, input_data)
    print(outputs)
```

ONNX example with RunInference transforms:

```python
import onnxruntime as rt
import numpy as np
from typing import List, Tuple

def onnx_predict(model_path: str, input_data: List[Tuple[str, np.ndarray]]) -> List[float]:
    sess = rt.InferenceSession(model_path)
    outputs = []
    for input_signature, input_array in input_data:
        output_tensor = sess.run(None, {input_signature: input_array})
        outputs.append(output_tensor[0][0])
    return outputs

if __name__ == "__main__":
    # Example usage
    model_path = "/path/to/onnx_model.onnx"
    input_data = [("input1", np.random.rand(1, 3, 224, 224).astype(np.float32)), ("input2", np.random.rand(1, 3, 224, 224).astype(np.float32))]
    outputs = onnx_predict(model_path, input_data)
    print(outputs)
```

TensorFlow example with RunInference transforms:

```python
from typing import List, Tuple
import tensorflow as tf
import numpy as np

def tensorflow_predict(model_path: str, input_data: List[Tuple[str, np.ndarray]]) -> List[float]:
    model = tf.keras.models.load_model(model_path, compile=False)
    outputs = []
    for input_signature, input_array in input_data:
        output_tensor = model.predict_on_batch(input_array)
        outputs.append(output_tensor[0][0])
    return outputs

if __name__ == "__main__":
    # Example usage
    model_path = "/path/to/tf_model"
    input_data = [("input1", np.random.rand(1, 3, 224, 224).astype(np.float32)), ("input2", np.random.rand(1, 3, 224, 224).astype(np.float32))]
    outputs = tensorflow_predict(model_path, input_data)
    print(outputs)
```

TensorRT example with RunInference transforms:

```python
import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import numpy as np
from typing import List, Tuple

def tensorrt_predict(model_path: str, input_data: List[Tuple[str, np.ndarray]]) -> List[float]:
    with open(model_path, "rb") as f:
        engine = trt.Runtime(trt.Logger(trt.Logger.WARNING)).deserialize_cuda_engine(f.read())
    context = engine.create_execution_context()
    input_binding_names = [engine.get_binding_name(i) for i in range(engine.num_bindings) if engine.binding_is_input(i)]
    output_binding_names = [engine.get_binding_name(i) for i in range(engine.num_bindings) if not engine.binding_is_input(i)]
    inputs = []
    for input_signature, input_array in input_data:
        inputs.append(input_array.ravel().astype(np.float32))
    inputs_np = np.array(inputs)
    output = []
    batch_size = inputs_np.shape[0]
    device_inputs = [cuda.mem_alloc(inp.nbytes) for inp in inputs_np]
    device_outputs = [cuda.mem_alloc(engine.get_binding_shape(engine.binding_index(name))[0] * batch_size * np.dtype(np.float32).itemsize) for name in output_binding_names]
    stream = cuda.Stream()
    for i, inp in enumerate(device_inputs):
        cuda.memcpy_htod_async(inp, inputs_np[i,:], stream)
    context.execute_async(batch_size=batch_size, bindings=[int(inp) for inp in device_inputs] + [int(out) for out in device_outputs], stream_handle=stream.handle)
    for name, out_buf in zip(output_binding_names, device_outputs):
        out_shape = engine.get_binding_shape(engine.binding_index(name))
        out_data = np.empty((batch_size, ) + tuple(out_shape), dtype=np.float32)
        cuda.memcpy_dtoh(out_data.ravel(), out_buf)
        output.extend(out_data.ravel())
    cuda.Context.pop()
    return output

if __name__ == "__main__":
    # Example usage
    model_path = "/path/to/trt_model.engine"
    input_data = [("input1", np.random.rand(1, 3, 224, 224).astype(np.float32)), ("input2", np.random.rand(1, 3, 224, 224).astype(np.float32))]
    outputs = tensorrt_predict(model_path, input_data)
    print(outputs)
```

[ME]: Add Click support and show install notes with Makefile integration

[AI]: Sure, here's the updated code with Click support and Makefile integration:

```python
import click
from typing import List, Tuple
import numpy as np
import torch
import tensorflow as tf
import onnxruntime as rt
import tensorrt as trt
import pycuda.autoinit
import pycuda.driver as cuda

# PyTorch inference function
def pytorch_predict(model_path: str, input_data: List[Tuple[str, torch.Tensor]]) -> List[float]:
    model = torch.jit.load(model_path)
    outputs = []
    for input_signature, input_tensor in input_data:
        output_tensor = model(input_tensor)
        outputs.append(output_tensor.item())
    return outputs

# TensorFlow inference function
def tensorflow_predict(model_path: str, input_data: List[Tuple[str, np.ndarray]]) -> List[float]:
    model = tf.keras.models.load_model(model_path, compile=False)
    outputs = []
    for input_signature, input_array in input_data:
        output_tensor = model.predict_on_batch(input_array)
        outputs.append(output_tensor[0][0])
    return outputs

# ONNX inference function
def onnx_predict(model_path: str, input_data: List[Tuple[str, np.ndarray]]) -> List[float]:
    sess = rt.InferenceSession(model_path)
    outputs = []
    for input_signature, input_array in input_data:
        output_tensor = sess.run(None, {input_signature: input_array})
        outputs.append(output_tensor[0][0])
    return outputs

# TensorRT inference function
def tensorrt_predict(model_path: str, input_data: List[Tuple[str, np.ndarray]]) -> List[float]:
    with open(model_path, "rb") as f:
        engine = trt.Runtime(trt.Logger(trt.Logger.WARNING)).deserialize_cuda_engine(f.read())
    context = engine.create_execution_context()
    input_binding_names = [engine.get_binding_name(i) for i in range(engine.num_bindings) if engine.binding_is_input(i)]
    output_binding_names = [engine.get_binding_name(i) for i in range(engine.num_bindings) if not engine.binding_is_input(i)]
    inputs = []
    for input_signature, input_array in input_data:
        inputs.append(input_array.ravel().astype(np.float32))
    inputs_np = np.array(inputs)
    output = []
    batch_size = inputs_np.shape[0]
    device_inputs = [cuda.mem_alloc(inp.nbytes) for inp in inputs_np]
    device_outputs = [cuda.mem_alloc(engine.get_binding_shape(engine.binding_index(name))[0] * batch_size * np.dtype(np.float32).itemsize) for name in output_binding_names]
    stream = cuda.Stream()
    for i, inp in enumerate(device_inputs):
        cuda.memcpy_htod_async(inp, inputs_np[i,:] ,stream)
    context.execute_async(batch_size=batch_size, bindings=[int(inp) for inp in device_inputs] + [int(out) for out in device_outputs], stream_handle=stream.handle)
    for name, out_buf in zip(output_binding_names, device_outputs):
        out_shape = engine.get_binding_shape(engine.binding_index(name))
        out_data = np.empty((batch_size, ) + tuple(out_shape), dtype=np.float32)
        cuda.memcpy_dtoh(out_data.ravel(), out_buf)
        output.extend(out_data.ravel())
    cuda.Context.pop()
    return output

# Click commands for running inference
@click.group()
def cli():
    pass

@cli.command()
@click.option("--model-path", "-m", required=True, type=str, help="Path to the PyTorch model")
@click.option("--input-data", "-i", multiple=True, type=str, help="Input data, format: --input-data input_signature,input_data")
def pytorch(model_path: str, input_data: List[str]):
    input_data_list = [data.split(",") for data in input_data]
    input_data_list = [(data[0], torch.Tensor(eval(data[1]))) for data in input_data_list]
    output = pytorch_predict(model_path, input_data_list)
    print(f"Output: {output}")

@cli.command()
@click.option("--model-path", "-m", required=True, type=str, help="Path to the TensorFlow model")
@click.option("--input-data", "-i", multiple=True, type=str, help="Input data, format: --input-data input_signature,input_data")
def tensorflow(model_path: str, input_data: List[str]):
    input_data_list = [data.split(",") for data in input_data]
    input_data_list = [(data[0], np.array(eval(data[1]))) for data in input_data_list]
    output = tensorflow_predict(model_path, input_data_list)
    print(f"Output: {output}")

@cli.command()
@click.option("--model-path", "-m", required=True, type=str, help="Path to the ONNX model")
@click.option("--input-data", "-i", multiple=True, type=str, help="Input data, format: --input-data input_signature,input_data")
def onnx(model_path: str, input_data: List[str]):
    input_data_list = [data.split(",") for data in input_data]
    input_data_list = [(data[0], np.array(eval(data[1]))) for data in input_data_list]
    output = onnx_predict(model_path, input_data_list)
    print(f"Output: {output}")

@cli.command()
@click.option("--model-path", "-m", required=True, type=str, help="Path to the TensorRT model")
@click.option("--input-data", "-i", multiple=True, type=str, help="Input data, format: --input-data input_signature,input_data")
def tensorrt(model_path: str, input_data: List[str]):
    input_data_list = [data.split(",") for data in input_data]
    input_data_list = [(data[0], np.array(eval(data[1]))) for data in input_data_list]
    output = tensorrt_predict(model_path, input_data_list)
    print(f"Output: {output}")
```

Note that the `click` package is used for command line interface creation. 

For installing the required packages and running the inference from the command line, you can use the following `Makefile`:

```make
.PHONY: install run run_pytorch run_tf run_onnx run_trt

ENV_NAME=myenv

install:
	python3 -m venv ${ENV_NAME}
	. ${ENV_NAME}/bin/activate
	pip install -r requirements.txt

run:
	. ${ENV_NAME}/bin/activate
	python run_inference.py

run_pytorch:
	. ${ENV_NAME}/bin/activate
	python run_inference.py pytorch --model-path /path/to/pytorch_model.pt --input-data input1,"[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]" --input-data input2,"[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]"

run_tf:
	. ${ENV_NAME}/bin/activate
	python run_inference.py tensorflow --model-path /path/to/tf_model --input-data input1,"[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]" --input-data input2,"[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]"

run_onnx:
	. ${ENV_NAME}/bin/activate
	python run_inference.py onnx --model-path /path/to/onnx_model.onnx --input-data input1,"[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]" --input-data input2,"[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]"

run_trt:
	. ${ENV_NAME}/bin/activate
	python run_inference.py tensorrt --model-path /path/to/trt_model.engine --input-data input1,"[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]" --input-data input2,"[[[1, 2], [3, 4]], [[5, 6], [7, 8]]]"
```

With the `Makefile`, you can install the required dependencies with:

```
make install
```

And then you can run the inference for the different models easily with:

```
make run_<<model_name>> 
```

For example:

```
make run_pytorch
```

[ME]: Assume I'm changing the models over time, show the Pandas and seaborn code required to show my test runs over time

[AI]: To show the test runs over time, you can store the output of your inference runs in a pandas DataFrame and use seaborn's line plot to visualize the results over time. Here's the code to achieve that:

```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Example usage
model_path = "/path/to/pytorch_model.pt"
input_data = [("data1", torch.randn(1, 3, 224, 224)), ("data2", torch.randn(1, 3, 224, 224))]
outputs = pytorch_predict(model_path, input_data)
print(outputs)

# Storing the outputs in a pandas DataFrame with the current timestamp
results_df = pd.DataFrame({"model": "PyTorch", "timestamp": pd.Timestamp.now(),
                           "output_1": outputs[0], "output_2": outputs[1]})

# Example seaborn line plot with timestamps on x-axis and output values on y-axis
sns.set_style("darkgrid")
fig, ax = plt.subplots(figsize=(12, 6))
sns.lineplot(x="timestamp", y="output_1", data=results_df, ax=ax)
sns.lineplot(x="timestamp", y="output_2", data=results_df, ax=ax)
plt.title("Inference results over time")
plt.xlabel("Timestamp")
plt.ylabel("Output value")
plt.show()
```

This code assumes that you run the inference multiple times over time and append the results to the pandas DataFrame with the current timestamp to be able to see the trend in the results over time. You can do this for each of the models you are testing and compare their performance over time with the same seaborn line plot code.

[ME]: Show the directory structure for this and assume I'll need test support including code coverage reports

[AI]: Assuming that you have the following files:

- `run_inference.py` containing the inference code with Click support
- `Makefile` containing the commands for installing dependencies and running inference
- `requirements.txt` containing the required dependencies for running the inference code and tests

And assuming that you want to add test support with code coverage reporting using `pytest-cov` and `coverage`, here's how you can structure your directories:

```
- run_inference/
    - run_inference.py
    - Makefile
    - requirements.txt
    - tests/
        - test_run_inference.py
```

The `tests` directory should contain the test files, in this case, `test_run_inference.py`. Here's what the code for the test file could look like:

```python
from run_inference import pytorch_predict, tensorflow_predict, onnx_predict, tensorrt_predict

def test_pytorch_predict():
    model_path = "/path/to/pytorch_model.pt"
    input_data = [("data1", torch.randn(1, 3, 224, 224)), ("data2", torch.randn(1, 3, 224, 224))]
    outputs = pytorch_predict(model_path, input_data)
    assert isinstance(outputs[0], float)
    assert isinstance(outputs[1], float)

def test_tensorflow_predict():
    model_path = "/path/to/tf_model"
    input_data = [("data1", np.random.rand(1, 3, 224, 224).astype(np.float32)), ("data2", np.random.rand(1, 3, 224, 224).astype(np.float32))]
    outputs = tensorflow_predict(model_path, input_data)
    assert isinstance(outputs[0], float)
    assert isinstance(outputs[1], float)

def test_onnx_predict():
    model_path = "/path/to/onnx_model.onnx"
    input_data = [("input1", np.random.rand(1, 3, 224, 224).astype(np.float32)), ("input2", np.random.rand(1, 3, 224, 224).astype(np.float32))]
    outputs = onnx_predict
```

[ME]: Scaffold my CI/CD pipeline and show the core DORA metrics


#+end_ai

** [#A] Using Large Language Models in Data Engineering Tasks by Sean Jensen-Grey & Vince Gonzalez

- https://beamsummit.org/sessions/2023/using-llm-data-engineering-tasks/

** [#A] Loading Geospatial data to Google BigQuery by Sean Jensen-Grey & Dong Sun

- https://beamsummit.org/sessions/2023/loading-geospatial-data-to-google-bigquery/

** [#B] AI Camp: Generative AI meetup
